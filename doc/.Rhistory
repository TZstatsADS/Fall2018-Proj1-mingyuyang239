library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(tm)
library(wordcloud)
library(syuzhet)
library(topicmodels)
library(ggplot2)
library(dplyr)
library(beeswarm)
library(gplots)
R.Version()$version.string
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
hm_data.sp <- hm_data[hm_data$marital == "single" & hm_data$parenthood == "y",]
hm_data.mp <- hm_data[hm_data$marital == "married" & hm_data$parenthood == "y",]
hm_data.cp <- rbind(hm_data.s,hm_data.m)
hm_data.sp <- hm_data[hm_data$marital == "single" & hm_data$parenthood == "y",]
hm_data.mp <- hm_data[hm_data$marital == "married" & hm_data$parenthood == "y",]
hm_data.cp <- rbind(hm_data.sp,hm_data.mp)
hm_data.s <- hm_data[hm_data$marital == "single" & hm_data$parenthood == "n",]
hm_data.m <- hm_data[hm_data$marital == "married" & hm_data$parenthood == "n",]
hm_data.c <- rbind(hm_data.s,hm_data.m)
dim(hm_data.sp)
dim(hm_data.mp)
dim(hm_data.cp )
dim(hm_data.s)
dim(hm_data.m)
dim(hm_data.c )
library(shiny)
library(plotly)
ggplot(alldata, aes(status, count, color = status)) +
geom_point() +
geom_jitter(width = 1, height = 1) +
ggtitle("Length of Sentences for Four Groups")
#wordcloud for single parents.
docs.sp <- Corpus(VectorSource(hm_data.sp$text))
dtm.sp <- TermDocumentMatrix(docs.sp)
m.sp <- as.matrix(dtm.sp)
v.sp <- sort(rowSums(m.sp),decreasing=TRUE)
d.sp <- data.frame(word = names(v.sp),freq=v.sp)
set.seed(123)
wordcloud(words = d.sp$word, freq = d.sp$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
#wordcloud for married parents
docs.mp <- Corpus(VectorSource(hm_data.mp$text))
dtm.mp <- TermDocumentMatrix(docs.mp)
m.mp <- as.matrix(dtm.mp)
v.mp <- sort(rowSums(m.mp),decreasing=TRUE)
d.mp <- data.frame(word = names(v.mp),freq=v.mp)
set.seed(123)
wordcloud(words = d.mp$word, freq = d.mp$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# all samples vs. single parents vs. married parents
par(mfrow = c(1,3))
sentiment.all <- get_sentiment(hm_data$text, method = "syuzhet")
order.data.cp <- hm_data.cp[order(hm_data.cp$wid),]
order.data.cp$status <- ifelse(order.data.cp$marital == "single", 1, 2)
order.data.c <- hm_data.c[order(hm_data.c$wid),]
order.data.c$status <- ifelse(order.data.c$marital == "single", 3, 4)
combine <- rbind(order.data.cp,order.data.c)
combine.data <- combine  %>%
group_by(wid) %>%
summarise(text = paste(text, collapse = " "), status = mean(status))
combine.docs <- Corpus(VectorSource(combine.data$text))
dtm.combine <- DocumentTermMatrix(combine.docs)
lda <- LDA(dtm.combine, k = 3, control = list(seed = 1234))
lda
topics <- tidy(lda, matrix = c("beta", "gamma"))
topics
top.words <- topics %>%
group_by(topic) %>%
top_n(5, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top.words %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
library(shiny)
library(plotly)
hm_data.cp$status <- ifelse(hm_data.cp$marital == "single", "Single Parents", "Married Parents")
hm_data.c$status <-ifelse(hm_data.c$marital == "single", "Single Non-parents", "Married Non-parents")
alldata <- rbind(hm_data.cp,hm_data.c)
ggplot(alldata, aes(status, count, color = status)) +
geom_point() +
geom_jitter(width = 1, height = 1) +
ggtitle("Length of Sentences for Four Groups")
